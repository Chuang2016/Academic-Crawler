#  项目说明

该项目目标：
- 采用爬虫、自然语言处理、机器学习的办法，快速获取某领域关键论文（PubMed，ScienceDirect）
- 使用自动化脚本，追踪领域关键论文相关趋势
- 采用人工智能分析 + 人工打分的办法，自动判断某篇论文“是否有用”
------
1. 建立爬虫环境

`project.py  webUI.py  spider_pm.py  task.py  run_logger.py`

  * [ ] 项目定时启动、关闭，运行自动化
  * [ ] 根据统计数据设定运行计划
  * [x] 采用selenium + phantomjs方案解决动态加载问题
  * [ ] 页面解析采用lxml xpath方案，加快速度
  * [x] 建立代理IP池，防止禁止访问：自动抓取IP，测试，更新
  * [ ] 采用反反爬虫机制：换user-agent，运行时间机制（夜间访问，间隔访问）
  * [ ] 建立多线程爬虫
  * [ ] 分布式爬虫
  * [ ] 建立VPN，使用校园IP下载指定全文（可以下载大部分PDF）
  * [ ] 采用flask搭建可视化界面，通过网页对项目进行监控
-----
2. 自动根据关键词、关键词组合或者搜索符等工作抓取PubMed上相关文献信息

`spider_pm.py journal.py institute.py`

  * [x] 抓取：题目、期刊、年份、作者、机构、摘要、关键词、全文链接
  * [x] 自动补齐：期刊影响因子、杂志分区信息
  * [ ] 自动补齐：该机构在全球范围的影响力或者排名
  * [ ] 部分关键论文查找引用率
  * [ ] 根据信息和简单的模型计算论文的重要性（不含人工智能或机器学习）
-----
3. 储存抓取回到的信息

`data_handler.py database.py`

  * [x] 标准储存在CSV文件中，阅读友好
  * [x] 储存改为MongoDB
  * [ ] Redis作为中间件
-----

4. 对抓取运行情况做统计分析，以便更好设计抓取任务

`stats.py`

  * [ ] 对抓取效率、错误做统计分析
  * [ ] 对每个关键词/关键词组合的产量做统计分析
  * [ ] 对关键词做自动优化，生成最优关键词组合
  * [ ] 对关键词的重复率做分析
-----

5. 对抓取回的文本信息做清洗，文本强化，便于阅读

`data_wash.py`

  * [x] 去除所有标签；清除特殊字符或者非英文字母；合并段落
  * [ ] 搜索关键词突出：包含本项目所涉及所有关键词；自定义关键词突出：自定义添加关注词，机构中国家、机构名突出
  * [ ] 根据自然语言处理词频分析，自动添加高频关键词
-----
6. 输出成阅读格式：MD、HTML静态文件，web输出, 屏幕输出

`output.py`

  * [ ] 使用jinja2生成MD，HTML静态文件
  * [ ] 使用flask实现动态查询输出
  * [ ] 通过网页对项目、条目内容进行修改
-----
7. 自然语言处理: NLTK工具
  * [ ] 文本清洗：停用词，词格
  * [ ] 手动加入行业关键词
  * [ ] 分词
  * [ ] 词频统计与向量分析
  * [ ] 对机构名进行分词，统计，归类同一机构
  * [ ] 多文章聚类分析，无监督学习，找到代表性论文
  * [ ] 关键信息提取
  * [ ] 自动文摘
-----
8. 深度学习模型建立：根据人工打分监督训练
  * [ ] 搭建文本、期刊资质、团队资质、作者资质等与人工打分之间联系的模型
  * [ ] 根据人工评分重新计算文章分数，重新排序
  * [ ] 根据人工智能排出最重要的国家、机构、期刊等信息
-----
9. 阅读后人工评分、更新
  * [ ] 人工多维度评论文章：主题相关度、文章质量、文章有用性、研究类型
  * [ ] 人工选择论文分类
  * [ ] 对重排的文章进行打分
  * [ ] 手动添加关键词
-----  
10. 根据建立的模型，提供每篇文章有用性的可能性